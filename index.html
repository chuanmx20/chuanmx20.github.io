<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/cc-32x32.svg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/cc-32x32.svg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/cc-16x16.svg">
  <link rel="mask-icon" href="/images/cc-128x128.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.chuanmx.cc","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="chuanmx的个人博客">
<meta property="og:url" content="https://www.chuanmx.cc/index.html">
<meta property="og:site_name" content="chuanmx的个人博客">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="chuanmx">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://www.chuanmx.cc/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>chuanmx的个人博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

  <script src="https://apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script><!--需要JQuery的支持-->
  <script type="text/javascript" src ="/js/mouse-click.js" ></script>
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --></head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">chuanmx的个人博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Note meaningful things down</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/chuanmx20" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.chuanmx.cc/2024/11/09/Policy-Gradient%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="chuanmx">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chuanmx的个人博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/11/09/Policy-Gradient%E7%AC%94%E8%AE%B0/" class="post-title-link" itemprop="url">Policy Gradient笔记</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2024-11-09 01:29:12 / 修改时间：01:34:46" itemprop="dateCreated datePublished" datetime="2024-11-09T01:29:12+08:00">2024-11-09</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="思从何来">思从何来</h2>
<p>我尝试理解RLHF（Reinforcement Learning with Human
Feedback），意识到我认知中的Q-Learning与当前RL有很大的出入，于是我学习了一下一些Deep
Q-Learning的知识，记录一下学习的过程。</p>
<h2 id="我认知中的rl">我认知中的RL</h2>
<p>RL最通俗而最通用的解释就是这么一个模型： 1.
整个系统中有Agent、Envronment两个部分 2.
通过观测Environment得到状态<span class="math display">\[s_t\]</span> 3.
Agent根据<span class="math display">\[s_t\]</span>选择最合适的动作<span
class="math display">\[a_t\]</span> 4. 动作<span
class="math display">\[a_t\]</span>作用于Environment，得到奖励<span
class="math display">\[r_{t+1}\]</span>和新的状态<span
class="math display">\[s_{t+1}\]</span> 5. Agent根据<span
class="math display">\[r_{t+1}\]</span>和<span
class="math display">\[s_{t+1}\]</span>更新自己的策略 6.
重复2-5步骤直至结束</p>
<p>对于每一个结局，我们都可以得到一个回报<span
class="math display">\[R=\sum_{t=0}^{\infty}\gamma^t
r_t\]</span>，其中<span
class="math display">\[\gamma\]</span>是折扣因子(discount
factor)，用于衡量未来奖励的重要性。
而整个强化学习的目标就是找到一个策略<span
class="math display">\[\pi\]</span>，使得期望回报最大。</p>
<p><span class="math display">\[
\pi^* = \arg\max_{\pi}E[R|\pi]
\]</span></p>
<h3 id="tabular-q-learning">Tabular Q-Learning</h3>
<p>从当前的状态<span
class="math display">\[s\]</span>出发，我们如何选择最优的动作<span
class="math display">\[a\]</span>，使得未来的回报最大呢？最容易想到的是贪心算法，即每一步都选择能够获得最大回报的动作。</p>
<p>贪心算法的问题在于，我们可能会陷入局部最优，而无法找到全局最优。为了解决这个问题，我们需要衡量每一个动作的价值，这个价值是一个综合考虑了当前动作的回报和未来动作的价值的一个值，那么我们就可以在每一步选择价值最大的动作，而不是直接选择回报最大的动作。</p>
<p>所以我们定义一个Q值，表示在状态<span
class="math display">\[s\]</span>下，选择动作<span
class="math display">\[a\]</span>的价值，即<span
class="math display">\[Q(s,a)\]</span>。假设action space和state
space都是有限的，那么我们可以用一个表格来存储Q值，表第<span
class="math inline">\(s\)</span>行第<span
class="math inline">\(a\)</span>列的值就是<span
class="math display">\[Q(s,a)\]</span>，表示在状态<span
class="math inline">\(s\)</span>下选择动作<span
class="math display">\[a\]</span>的价值。</p>
<p>有了这么一张表，我们只需要在每一个状态下选择最大的Q值对应的动作，就能得到一个全局最优策略。注意，这里指的是全局最优，而不是局部最优，因为我们的Q值是全局的，而不是局部的。获得这样一张表的方法就是Q-Learning，即学习每一个<span
class="math display">\[(s,a)\]</span>对的Q值，使得最终的Q值表能够收敛到全局最优。</p>
<p>Q-Learning的更新公式如下： <span class="math display">\[
Q^{k+1}(s_t,a_t) = Q^k(s_t,a_t) +
\alpha(r_{t+1}+\gamma\max_{a}Q^k(s_{t+1},a)-Q^k(s_t,a_t))
\]</span> 其中<span
class="math inline">\(\alpha\)</span>是学习率，用于控制每次更新的幅度，<span
class="math inline">\(Q^k\)</span>表示第<span
class="math inline">\(k\)</span>次迭代的Q值表。可以这样理解这个更新公式：<span
class="math display">\[Q^{k+1}\]</span>是<span
class="math display">\[Q^k\]</span>的一个修正，修正的幅度由<span
class="math display">\[r_{t+1}+\gamma\max_{a}Q^k(s_{t+1},a)-Q^k(s_t,a_t)\]</span>决定。</p>
<p>更新的增量的几个项分别表示： 1. <span
class="math display">\[r_{t+1}\]</span>表示当前状态<span
class="math inline">\(s_t\)</span>选择动作<span
class="math display">\[a_t\]</span>后得到的奖励 2. <span
class="math display">\[\gamma\]</span>乘的这一项表示在下一个状态<span
class="math display">\[s_{t+1}\]</span>下选择最优动作的Q值。也就是对<span
class="math inline">\(s_t\)</span>选择<span
class="math inline">\(a_t\)</span>后，接下来的最优策略的价值的一个估计
3. <span class="math display">\[Q^k(s_t,a_t)\]</span>表示当前状态<span
class="math display">\[s_t\]</span>选择动作<span
class="math inline">\(a_t\)</span>的Q值，也就是当前策略的价值 4.
2-3项的差值表示当前策略和最优策略的差距，代表走这个动作能够获得的额外价值</p>
<p>与Superveised
Learning不同，RL的目标是最大化回报，而不是最小化损失。此外RL的训练可以理解为遍历整个状态空间，而不是遍历整个数据集。这也是为什么RL的训练速度会比SL慢的原因。它停止的条件是策略收敛，也就是<span
class="math display">\[|Q^{k+1} - Q^k| = \epsilon\]</span>，当<span
class="math display">\[\epsilon\]</span>足够小时，我们就可以认为Q值表已经收敛。但是RL并不能保证收敛，因为Q值表的更新是基于贪心策略的，而贪心策略可能会陷入局部最优。
这一优化过程可以等价于最小化增量的平方和： <span class="math display">\[
\min_{Q} \sum_{t=0}^{\infty}(Q^{k+1}(s_t,a_t) - Q^k(s_t,a_t))^2
\]</span></p>
<p>这就是我认知中的Q-Learning，但是这个Q-Learning有一个很大的问题，那就是它只能处理离散的动作空间，而不能处理连续的动作空间。而且，它最优化的终点并不是Q值表保证每个<span
class="math display">\[(s,a)\]</span>对的Q值都能代表最优策略，而是Q值表的收敛。</p>
<h2 id="deep-q-learning">Deep Q-Learning</h2>
<p>刚刚提到的TQL，有很大的局限性，那就是只能处理离散的动作空间。而这一问题的根源在于TQL的核心是Q
table，而Q
table是一个离散的表格，所以只能处理离散的动作空间。那如何能对连续空间的<span
class="math display">\[(s,a)\]</span>对进行Q值的估计呢？这就是DQL的出发点。</p>
<h3 id="parametrize-q-function">Parametrize Q function</h3>
<p>DQL的核心思想是，用一个函数来估计Q值，而不是用一个表格。我们用<span
class="math inline">\(\theta\)</span>来表示这个函数的参数，那么对于任意的<span
class="math display">\[(s,a)\]</span>对，我们可以用<span
class="math display">\[Q(s,a;\theta) \equiv
Q_{\theta}(s,a)\]</span>来计算Q值。</p>
<h3 id="dqndeep-q-learning-network">DQN（Deep Q-learning Network）</h3>
<p><img src="/images/2024-11-09-Policy-Gradient笔记/Picture1.png"
alt="PingPong Game" />
假设有这样一款游戏，你的Agent需要操控右边的板子，选择向上还是向下移动，使得板子可以接住小球，而左边的板子是你的对手，你的目标是尽可能多的接住小球。这个游戏的状态空间是非常大，因为两块板子和小球的位置都是连续的，而动作空间是离散的，因为只有向上和向下两个动作。</p>
<p>对于这么大的状态空间而言，学习这样的一个Q
table是不现实的。我们可以设计这么一个网络来计算Q值，从而选择向上还是向下。我们把状态建模成这么几个特征：
1. 小球的位置<span class="math display">\[x_{ball}\]</span> <span
class="math display">\[y_{ball}\]</span> 2. 对手板子的位置<span
class="math display">\[y_{opponent}\]</span> 3. 你的板子的位置<span
class="math display">\[y_{agent}\]</span> 4. 小球的速度<span
class="math display">\[v^x_{ball}\]</span> <span
class="math display">\[v^y_{ball}\]</span></p>
<p>简单设计这样一个网络 <img
src="/images/2024-11-09-Policy-Gradient笔记/Picture1-1.png"
alt="DQL Network" />
这个Q函数得到的结果是一个0-1之间的sigmoid值，表示向上（或者向下，这里只有两个动作，所以是相对的）的概率。通过优化这个网络的参数<span
class="math display">\[\theta\]</span>，我们就可以得到一个最优的Q函数，使得在任意的状态下，选择最大的Q值对应的动作，就能得到一个全局最优的策略。有了这个网络，接下来要讨论的是，如何训练这个网络。</p>
<h3 id="policy-gradient">Policy Gradient</h3>
<p>优化这个<span class="math display">\[\theta\]</span>
function的核心依然是，使得这个<span
class="math display">\[\theta\]</span>
function的期望回报最大，这里就提到了两个东西： 1. 回报 2. 期望</p>
<p>回报很容易理解，我们做如下定义，从状态<span
class="math inline">\(s_0\)</span>开始，经过一系列的动作<span
class="math display">\[a_0,a_1,...,a_n\]</span>游戏结束，对每一步<span
class="math display">\[a_i\]</span>，我们都会得到一个奖励<span
class="math display">\[r_i\]</span>，将agent与环境交互的这一整个过程称为一条轨迹(trajectory)，用符号<span
class="math display">\[\tau\]</span>表示轨迹。那么这一条轨迹的回报就是
<span class="math display">\[
R(\tau) = \sum_{t=0}^{n}\gamma^t r_t
\]</span> 其中<span
class="math display">\[\gamma\]</span>是折扣因子，是一个超参数，用于衡量未来奖励的重要性。而参数<span
class="math display">\[\theta\]</span>的期望回报可以用经验分布的期望来计算，使用<span
class="math inline">\(\theta\)</span>的策略<span
class="math inline">\(\pi_{\theta}\)</span>采样一系列的轨迹，然后计算这些轨迹的回报的期望值，即
<span class="math display">\[
E[R(\tau)] = \sum_{\tau}P_{\theta}(\tau)R(\tau)
\]</span> 其中<span
class="math display">\[P(\tau)\]</span>是轨迹的概率分布，是从<span
class="math inline">\(\theta\)</span>的策略<span
class="math display">\[\pi_{\theta}\]</span>采样得到的轨迹的分布，也就是一系列<span
class="math display">\[(s,a)\]</span>对概率的乘积，表示从<span
class="math inline">\(s_0\)</span>开始，经过一系列的动作<span
class="math inline">\(a_0,a_1,...,a_n\)</span>游戏结束的概率，即 <span
class="math display">\[
P(\tau) = \prod_{t=0}^{n}P_{\theta}(a_t|s_t)
\]</span></p>
<p>假设采样次数足够多，那么我们可以认为<span
class="math display">\[\theta\]</span>的策略<span
class="math display">\[\pi_\theta\]</span>为从<span
class="math display">\[s_0\]</span>出发到游戏结束的轨迹的分布(<span
class="math display">\[\tau\sim\pi_\theta\]</span>)。期望回报也可以写出轨迹回报的积分形式
<span class="math display">\[
E_{\tau\sim \pi_{\theta}}[R(\tau)] = \int P_{\theta}(\tau)R(\tau)d\tau
\]</span>
到这里，整个训练的目标就十分清晰了，我们的目标是最大化期望回报，即 <span
class="math display">\[
\theta^* = \arg\max_{\theta}E_{\tau\sim P_{\theta}(\tau)}[R(\tau)]
\]</span></p>
<p>原理上理解了，而这个期望可以看作是一个关于<span
class="math display">\[\theta\]</span>的函数。找到让这个函数最大的<span
class="math display">\[\theta\]</span>，就是我们的目标。如何实现这一目标呢，（你想的对，牛顿法）我们可以用梯度上升法来实现这一目标。同时我们可以注意到，期望回报的两项中，<span
class="math display">\[P_{\theta}(\tau)\]</span>是关于<span
class="math display">\[\theta\]</span>的函数，而<span
class="math display">\[R(\tau)\]</span>的计算是不依赖于<span
class="math display">\[\theta\]</span>的，所以我们可以将期望回报的梯度进一步展开并化简</p>
<p><span class="math display">\[
\begin{aligned}
\nabla_{\theta}E_{\tau\sim P(\tau)}[R(\tau)] &amp;=
\nabla_{\theta}\sum_{\tau}P_{\theta}(\tau)R(\tau)\\
&amp;= \sum_{\tau}\nabla_{\theta}P_{\theta}(\tau)R(\tau)\\
&amp;= \sum_{\tau}R(\tau)\nabla_{\theta}P_{\theta}(\tau)\\
&amp;=
\sum_{\tau}R(\tau)\nabla_{\theta}\prod_{t=0}^{n}P_{\theta}(a_t|s_t)\\
&amp;\propto \sum_{\tau}R(\tau)\nabla_{\theta}\sum_{t=0}^{n}\log
P_{\theta}(a_t|s_t)
\end{aligned}
\]</span> 在展开的最后一步，我把对<span
class="math display">\[\tau\]</span>中每次状态转移的概率的求导展开成了对它的对数求导，这样一方面避免了累乘导致的数值消失，另一方面我们也可以对么一个<span
class="math display">\[\tau\]</span>中的每一个<span
class="math display">\[(s,a)\]</span>对的logit求导，这样就可以对每一个<span
class="math display">\[(s,a)\]</span>对的概率进行更新。</p>
<p>根据该梯度公式，我们可以按照如下步骤进行训练： 1. 初始化<span
class="math display">\[\theta\]</span> 2. 采样一系列的轨迹<span
class="math display">\[\pi_{\theta}\]</span> 3.
遍历这些轨迹，对每条轨迹<span class="math display">\[\tau \in
\pi_{\theta}\]</span>，计算<span class="math inline">\(R(\tau)\)</span>
4. 对<span class="math display">\[\tau\]</span>中对时间步<span
class="math display">\[t = 0,1,...,n\]</span>，做如下更新 1. 计算<span
class="math display">\[\nabla_{\theta}\log P_{\theta}(a_t|s_t)\]</span>
2. 更新<span class="math display">\[\theta = \theta + \alpha
R(\tau)\nabla_{\theta}\log P_{\theta}(a_t|s_t)\]</span> 5.
重复2步骤直至收敛</p>
<p>这就是Policy
Gradient的训练过程，这个过程的核心是计算期望回报的梯度，然后根据梯度更新参数<span
class="math display">\[\theta\]</span>。梯度计算中，只有每一个<span
class="math display">\[(s,a)\]</span>对的概率是关于<span
class="math display">\[\theta\]</span>的函数，而回报是不依赖于<span
class="math display">\[\theta\]</span>的，最优化的迭代方向就是对于<span
class="math display">\[R(\tau)\]</span>比较大的轨迹<span
class="math display">\[\tau\]</span>，增大这个轨迹的概率，对于<span
class="math display">\[R(\tau)\]</span>比较小的轨迹<span
class="math display">\[\tau\]</span>，减小这个轨迹的概率。</p>
<h2 id="rlhf-reinforcement-learning-with-human-feedback">RLHF:
Reinforcement Learning with Human Feedback</h2>
<blockquote>
<p>以下内容系个人理解，可能有误，欢迎指正</p>
</blockquote>
<p>观察刚刚提到的Policy
Gradient的训练过程，我们可以发现，这个过程是一个无监督的过程，它只需要环境的反馈（即对每一个轨迹的评分），而不需要一个标注好的数据集。</p>
<p>大语言模型的训练过程剧场常见的梯度下降，每次蒙上句子的下一个token，根据上下文生成每一个位置对应的下一个token的概率分布，使用交叉熵损失函数计算这个概率分布与真实token的差距，然后更新参数。</p>
<p>大语言模型补全句子的过程，也可以理解为不断与环境交互的过程，<span
class="math display">\[s_t\]</span>为t时刻的上下文，<span
class="math display">\[a_t\]</span>为对<span
class="math display">\[s_t\]</span>采取的token，取自<span
class="math display">\[a_t =
\max_{a}\pi_\theta(a|s_t)\]</span>，其中<span
class="math display">\[\pi_\theta(s_t)\]</span>为模型预测的所有action的一个概率分布，也就是每一个token的概率，<span
class="math display">\[\tau\]</span>就是整个生成完成的句子。</p>
<p>因为它的训练数据多是一些语料，比如新闻报道，维基百科，而并不是对话，受到这一数据集分布特点的影响，它补全句子倾向于让句子更加像训练数据集的概率分布，而不是对话的文本。这一点并不能满足AI
assistant的需求，因为你没法和这样的补全句子的模型进行对话。因此我们需要增强这个模型的对话能力，即让他回复的句子更加符合对话的语境。同时我们也希望它的价值取向更倾向于人类对话场景的价值取向，比如礼貌，逻辑性等。那如何提高它采用出符合对话语境的句子<span
class="math display">\[\tau\]</span>的概率呢？</p>
<h3 id="sft-supervised-fine-tuning">SFT: Supervised Fine-Tuning</h3>
<p>很容易想到，我们可以收集一个对话数据集，然后让这个模型在该数据集上进一步梯度下降，使得它的输出更加符合对话的语境。这个过程就是SFT，即在一个标注好的对话数据集上进行梯度下降，使得模型的输出更加符合对话的语境。</p>
<p>但是想象这么一个情况：</p>
<p>你希望它在补全“This is”的时候，说的话更像“This is a xxx”而不是“This
is gonna
xxx”，那标注数据中就可以让前者类型的语句更多，后者类型的语句更少。比如你的数据集里有“This
is an apple”，但是没有“This is a banana”，可以预见的是，生成“This is an
apple”的概率变大了，但是“This is a
banana”的概率不一定会变大，甚至可能会变小。因为在计算<span
class="math display">\[P_\theta(This \quad is\quad
a)\]</span>的时候，我们假设它有三个选择（book, apple,
banana），label是(0, 1, 0)，那么向损失函数（交叉熵）小的方向调整<span
class="math display">\[\theta\]</span>就会更倾向于让它生成apple，而不是banana。但是其实我们更希望要的是(0.33,
0.33, 0.33)，因为三个选择都是合理的，而不是让它生成apple的概率更大。</p>
<p>这与我们的目的有所出入，我们希望的是让它生成的句子更符合你的需要，即让他补全this
is的时候更倾向于表达这是一个什么东西（is
sth），而不是表达将会发生什么（gonna
happen）。而这样训练并不能达到这一效果，只是让它生成的句子更符合微调数据的分布。</p>
<h3 id="hf-human-feedback">HF: Human Feedback</h3>
<p>为了实现这一目的，就需要量化什么样的句子是“你想要的句子”，也就是对每个句子你得去评价它是不是你想要的。而这对每个句子的评价其实就是所谓的人类反馈（Human
Feedback）。当然，不能够是人去盯着对每个句子都打分，所以要搞一个方法来学习人的偏好，模拟人打分，即对Human
Preference的建模。</p>
<p>这里，openai提出的方案是使用标注好的语句评分数据集来有监督地训练一个打分模型<span
class="math display">\[R_\phi\]</span>，他对任何句子<span
class="math display">\[\tau\]</span>都可以给出一个分数<span
class="math display">\[R_\phi(\tau)\]</span>，这个分数表示这个句子符合你的期望的程度。那既然能够对每一个句子打分，那剩下的只需要让得分高的policy的概率变大，得分低的policy的概率变小就行了。这下就回到了我们上面提到的Policy
Gradient的训练过程，使用这个打分模型<span
class="math display">\[\phi\]</span>对预训练语言模型<span
class="math display">\[\theta\]</span>做强化学习： <span
class="math display">\[
\begin{aligned}
\arg\max_{\theta}E_{\tau\sim P_{\theta}(\tau)}[R_\phi(\tau)]
&amp;= \arg\max_{\theta}\sum_{\tau \in
\pi_\theta}P_{\theta}(\tau)R_\phi(\tau)\\
\end{aligned}
\]</span>
通过最大化句子reward和句子的概率的乘积，我们就可以实现让打分高的句子的概率变大，打分低的句子的概率变小的目的。不同的地方仅仅在于这个<span
class="math display">\[R_\phi(\tau)\]</span>是人类的偏好模型给出的，而不是环境的奖励。</p>
<p>当然RLHF还要更多的工作细节，包括了使用PPO（Proximal Policy
Optimization）算法来提高训练的稳定性，以及使用GPT-3作为预训练模型等等。但是核心思想就是这样，通过人类的反馈来指导模型的训练，使得模型的输出更加符合人类的期望。</p>
<h3 id="why-rl">Why RL?</h3>
<p>解释了RLHF的微调过程，不仅还是想问，即然有对每个句子的打分，为什么不直接用监督学习呢？这里我个人猜测核心原因在于：
1.
首先这个东西可以动态的调整模型。在使用ChatGPT的过程中常常会出现，它提供两个句子，你选一个符合你的偏好的然后继续聊天，这一次过程甚至可以被用于对模型微调（当然openai不可能这么直接用来微调，比较用户也不一定好好选）
2. 训练这个打分模型<span
class="math display">\[R_\phi\]</span>，可以捕获到token
embedding之间的相似性，比如数据集里给“This is an
apple”打了高分，那么由于apple和banana之间比较相近（embedding空间上），在训练好的<span
class="math display">\[R_\phi\]</span>上，给“This is a
banana”也会打相对高的分数，通过这个评分模型来强化学习训练<span
class="math display">\[\theta\]</span>可以使得<span
class="math display">\[\theta\]</span>更好的学习到人类的偏好，生成更加符合人类偏好的句子，而不是生成更符合微调数据集分布的句子。</p>
<h2 id="总结">总结</h2>
<p>这篇文章主要是记录了我对Policy
Gradient的理解，以及对RLHF的一些思考。从传统的TQL（Tabular
Q-Learning）介绍到DQL（Deep
Q-Learning），从目的出发理解DQL存在的意义。然后介绍了Policy
Gradient是参数化策略<span
class="math display">\[\pi_\theta\]</span>以<span
class="math display">\[s_0\]</span>为起点采样得到的轨迹的期望回报的梯度，并进一步对梯度进行了展开和化简，从而解释了迭代更新策略参数的一般化方法。最后介绍了RLHF的训练原理，以及我个人对为什么要用RLHF而不是直接用SFT的见解。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.chuanmx.cc/2024/10/25/QKV%E5%A5%87%E6%80%9D%E5%B0%8F%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="chuanmx">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="chuanmx的个人博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2024/10/25/QKV%E5%A5%87%E6%80%9D%E5%B0%8F%E8%AE%B0/" class="post-title-link" itemprop="url">QKV奇思小记</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2024-10-25 13:50:42 / 修改时间：22:46:50" itemprop="dateCreated datePublished" datetime="2024-10-25T13:50:42+08:00">2024-10-25</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">深度学习</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="思路起源"><a href="#思路起源" class="headerlink" title="思路起源"></a>思路起源</h2><p>最近碰巧需要研究使用LLM来处理一些数据，从一个研究AI4Network的哥们口中得知了一种用LLM做网络异常诊断的方法。该方法的核心思想是把大语言模型当做一个强大的序列处理工具，任由其对序列数据提取特征并做异常分类，简易流程图就像这样</p>
<pre class="mermaid">graph TD;

met1[吞吐量]
met2[带宽]
metdot[...]
met3[时延]

encode(编码)

met1 ---> encode
met2 ---> encode
metdot ---> encode
met3 ---> encode

tokens[token]

encode --> tokens</pre>

<pre class="mermaid">graph TD;
tokens[token]
embeddings[embeddings]
tokens --embedding--> embeddings
q[Q]
k[K]
v[V]
embeddings --Wq--> q
embeddings --Wk--> k
embeddings --Wv--> v

qs[q+PE]
ks[k+PE]

q --PosEmb--> qs

k --PosEmb--> ks

qk(MatMul)

qs --> qk
ks --> qk

weight[Weight]

qk --norm & Softmax--> weight

wv(MatMul)

v -------> wv
weight -->wv

wv --fc classification--> output
output</pre>

<p>简单来说主要是两部分：</p>
<ol>
<li>先将网络当中的各种指标整理成向量，每一个截面能收集一个向量，所以在时间轴上是一个矩阵，可以取过去一段时间的所有截面来作为输入。那第一步就是将这个向量编码成一个固定的格式，方便后续与模型对齐。</li>
<li>将编码后的token转成选用的语言模型使用的embedding维度（对齐）</li>
<li>将对齐后的向量输入LLM（此处我简要化成了一个SelfAttn块，因为这是本文要讨论的重点）</li>
<li>输出经过一个全连阶层得到几大异常分类，计算标签和预测值之间的交叉熵，最优化似然值的相反数来调整embedding、推理模型、fc分类器三个部分</li>
</ol>
<p>由于我不是做网络的，上述方案的可行性和有效性不属于我讨论的范畴，但是受此启发，我对QKV查询机制展开了一些联想，并与朋友们展开了一轮激烈的讨论</p>
<h2 id="QKV：哪里来的怪东西"><a href="#QKV：哪里来的怪东西" class="headerlink" title="QKV：哪里来的怪东西"></a>QKV：哪里来的怪东西</h2><p>以下的逻辑链条属于我自己的想法，可能与现实有所出入。</p>
<h3 id="KV？查表"><a href="#KV？查表" class="headerlink" title="KV？查表"></a>KV？查表</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">身高</th>
<th style="text-align:center">体重</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">150</td>
<td style="text-align:center">48</td>
</tr>
<tr>
<td style="text-align:center">160</td>
<td style="text-align:center">55</td>
</tr>
<tr>
<td style="text-align:center">170</td>
<td style="text-align:center">65</td>
</tr>
<tr>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
</tr>
<tr>
<td style="text-align:center">190</td>
<td style="text-align:center">80</td>
</tr>
</tbody>
</table>
</div>
<p>假设我有这么一张KV表（K是身高V是体重），我想知道一个身高为165的人体重大概是多少，应该如何在表中查？</p>
<h4 id="插值（interpolation）"><a href="#插值（interpolation）" class="headerlink" title="插值（interpolation）"></a>插值（interpolation）</h4><p>165落在[160, 170]这个区间里，那么我们可以根据160和170对应的体重来估计165对应的体重。那么最简单也是最直观的想法就是，我们假设160到170之间的体重是随身高线性变化的，那么165就是160和170体重的平均值，也就是所谓的线性插值。或者我们考虑体重的变化是一条抛物线（两个可以求解出无数条抛物线，任选一条都能取出一个预测的体重值来），等等。<br>插值的方法很直观，但是他有一个弊端就是，任由我们有一张超大的KV表，有这么多全局的信息，却只取了两个点，有点太浪费了</p>
<p>那有没有一种更好的方法，能够利用KV表的所有信息呢？</p>
<h4 id="回归（regression）"><a href="#回归（regression）" class="headerlink" title="回归（regression）"></a>回归（regression）</h4><p>基于先验信息（就是这一张表），我们可以挑选一个映射f，这个f负责把身高映射到体重：</p>
<script type="math/tex; mode=display">f(身高)=\hat{体重}</script><p>最简单的就是线性回归了，即$y=ax+b+\epsilon$，其中：</p>
<script type="math/tex; mode=display">
    \begin{cases}
      a=\frac{\sum(x_i-\bar{x})(y_i-\bar{y})}{\sum(x_i-\bar{x})^2}\\
      b=\bar{y}-a\bar{x}
    \end{cases}</script><p>这玩意怎么来的非常简单，主要就是最小二乘$\mathop{min}\limits_{a,b}\sum(\hat{y_i}-y_i)^2$，都是高中数学的部分我就不细说了。</p>
<p>除了线性回归也可以根据KV的具体分布，来对K或者V做指数或者对数，然后再做线性回归，这也就是所谓的指数回归和对数回归</p>
<p>回归过于关注全局的信息，往往会忽略局部的信息，比如Q于某几个K距离很近，那我们其实更希望这几个K对应的权重更大一些，有没有一种方法可以实现这一权重分配呢？</p>
<h4 id="距离加权平均值"><a href="#距离加权平均值" class="headerlink" title="距离加权平均值"></a>距离加权平均值</h4><p>我们能不能为所有的KV对都分配一个权重，然后对他们取加权平均呢？答案是肯定的，但是这也面临一个新的问题，<mark>权重该如何分配</mark>?</p>
<h5 id="距离定义"><a href="#距离定义" class="headerlink" title="距离定义"></a>距离定义</h5><p>首先我们定义距离，用$L(q, k_i)$表示q和$k_i$之间的距离。对于这个身高来说，距离本身是有方向的比别人高五厘米和比别人矮五厘米是不同的。考虑到我们希望用身高对应的key来预测q对应的体重，那么160和170对于165来说都是同等重要的（即距离应该是无向的）。</p>
<p>$L$就可以是$|q - k_i|$，$|q - k_i| ^2$，两者的区别在于，前者在$q = k_i$时不可导，后者定义域内可导，这都是后面再考虑的问题了。</p>
<h5 id="权重分配"><a href="#权重分配" class="headerlink" title="权重分配"></a>权重分配</h5><p>有了距离$L$，我们应该如何分配权重呢。不需要动脑就可以想到，诶，我可以用相反数或者导数啊</p>
<script type="math/tex; mode=display">
weight = -L(q, k_i)</script><p>或者</p>
<script type="math/tex; mode=display">
weight = \frac{1}{L(q,k_i)}</script><p>而这两个都各自有着很大的问题，那就是当$q=k_i$时的数值问题，前者在$q$刚好与$k_i$匹配的时候，分配给$k_i$的权重是0，那加权平均的时候就会跳过这一个kv对。而后者在这个点上行为未定义（1/0），也不合适。虽然对于科学计算而言1/0会给你返回一个infinity，对应一个浮点数，那么计算加权平均的时候就会是inf/inf，查询的结果就是这个匹配的kv对，但是这本身是不合理的。</p>
<p>这里大家常用的方法其实是<script type="math/tex">e^{-L}</script><br><img src="/images/2024-10-25-QKV奇思小记/image-1.png" alt="$e^{-L}$"><br>可以看到在指数函数很好的处理了这一个点的值的问题，而e指数还有一大特点就是他的导数也是他自身（这里也不太重要）</p>
<p>那我们就定义权重分配的函数</p>
<script type="math/tex; mode=display">
f(q) = e^{-(q - K)^2}</script><h5 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h5><p>这样的分配权重解决了数值范围的问题，将所有权重都分配到(0,1]之间了。不过这种方法分配的权重大小完全取决于距离函数的输出，导致距离较小的k权重比距离大的权重要大很多，使得某些v对预测的贡献量远大于其他，结果不够平滑,可以根据具体的应用场景选择使用归一化操作。如果你希望每个k对应的v都能有一定的贡献，就归一化；如果你希望直接依赖于距离的差异并对局部相似的k赋予较大权重，可以不归一化。</p>
<p>归一化的方法有很多，比如使用Softmax方法将权重转化为概率分布，所有权重的和为1，能够有效对比不同k的影响力。</p>
<script type="math/tex; mode=display">
Softmax(W) = \frac{e^{w_i}}{\sum\limits_{i}e^{w_i}}</script><p>另外，使用Softmax可以确保权重总和为1，且更好地分配权重，特别是当k、q的分布较为复杂时，可以防止极端值对结果的过度影响。</p>
<h3 id="查KV表的推广"><a href="#查KV表的推广" class="headerlink" title="查KV表的推广"></a>查KV表的推广</h3><h4 id="多行查询"><a href="#多行查询" class="headerlink" title="多行查询"></a>多行查询</h4><p>一次除了可以查一个身高体重，实际上也可以查多个身高对应的体重，也就是说</p>
<script type="math/tex; mode=display">
Q=[153,156,171,184]^T</script><p>Q的每一行都代表一次查询，假设有t条查询，有h个kv对，我们可以把Q扩展到$Q^*\in R^{t,h}$那么Q和K的距离就是</p>
<script type="math/tex; mode=display">
L(Q,K) = (Q^*)^2+(K^2)^T-2QK^T</script><p>得到一个t行h列的距离矩阵W，$W_{i,j}$表示$Q_i$和$K_j$的距离<br>那$Softmax(W， axis=1)*V$就能得到一个t行1列的值，每一个值就代表$Q_i$对应的查询结果。</p>
<h4 id="Q-i-和-Q-j-之间的关系"><a href="#Q-i-和-Q-j-之间的关系" class="headerlink" title="$Q_i$和$Q_j$之间的关系"></a>$Q_i$和$Q_j$之间的关系</h4><p>在查询的时候，不难发现$Q_i$和$Q_j$其实是相互独立的，对于身高的查询来说，这无可厚非，因为不同的查询，查询的位置都不应该影响查询的结果。但是如果我们换个场景，我们需要处理文本序列的情绪分类，那么下面两个句子只是两个词的位置调换一下，情感分类结果其实是大不相同的：</p>
<ol>
<li>I eat apple</li>
<li>Apple eats I</li>
</ol>
<p>如果只用前面提到的方法，那么三个词query到的值只是位置变换了，而值本身没有区别。那么按照前文的多值query的方法显然就是不行了，因为我们引入了查询Q的序的概念，在查询的过程中除了需要处理每个query本身的值，还需要考虑每个query的位置。描述每个query所在的位置，这就是<strong>位置编码（Position Embedding）</strong>。</p>
<h4 id="位置编码（PE：Position-Embedding）"><a href="#位置编码（PE：Position-Embedding）" class="headerlink" title="位置编码（PE：Position Embedding）"></a>位置编码（PE：Position Embedding）</h4><p>那对于一个Q来说，假设他是t行1列的查询向量，那位置编码其实就是对着t个位置进行一个编码，把每个位置的编码填入这么一个t行1列的表中，得到与Q形状相同的一个位置向量。</p>
<p>Transformer中通过简单的相加来让Q和K携带上位置信息：</p>
<script type="math/tex; mode=display">
Q\prime*K\prime ^T= (Q+PosEmb)(K+PosEmb)^T</script><p>这个想法很好理解，如果不加上位置编码，那么I eat Apple和Apple eats I就是简单的两行互换，没什么区别。而加上了位置编码，这两个句子就完全不同了，因为两个顺序下，Apple所加上的位置编码不同，所以两个句子中apple对应的值也不同。在数据层面体现出了两个句子的不同，剩下的就交给梯度下降就行了。</p>
<p>那对位置进行编码该如何设计，又有什么需要考虑的呢？以下两点是根本：</p>
<ol>
<li>需要保证元素之间的相对距离关系，比如$Q_1$和$Q_3$之间的距离比$Q_1$和$Q_2$的远</li>
<li>要尽可能避免不同位置之间的编码重复。 如果在某两行他们的编码完全相同，或者说距离非常近就会出现词语对调后语意不变的问题。假设第i行和第j行位置编码重复了，那意味着一个句子把i和j行对应的词互换一下，互换前后两个句子加上位置信息的值也没有区别，即Apple eats I = I eat apple，这就乱套了。</li>
</ol>
<h5 id="线性位置编码"><a href="#线性位置编码" class="headerlink" title="线性位置编码"></a>线性位置编码</h5><p>那为了解决上面这个问题，我们很容易想到，就用这个位置的下标作为编码，不就能解决这一问题了吗？<br>这也是最早期的设计方式之一，用下标作为编码，而这有个十分明显的问题，当你需要把这个位置编码加入到序列中时，因为位置编码本身是个无界的常数，如果直接与序列相加，过大的数值会干扰查询。</p>
<script type="math/tex; mode=display">
Q=[153,156,171,...,184]^T</script><script type="math/tex; mode=display">
EB=[1,2,...,140]</script><p>那越往后的位置，位置编码过大就会使得查询的权重构建方式异常的复杂。</p>
<p>解决这一现象也简单，我们让第一个元素为0，最后一个元素为1，中间就线性插值不就好了吗。</p>
<blockquote>
<p>[0, 0.25, 0.5, 0.75, 1]</p>
</blockquote>
<p>这也是比较合理的绝对位置编码方式，有效的控制了数值的范围，同时保留了距离的概念。</p>
<p>不过，由于Q的长度t是个变动的值，那对不同的t而言，每个位置的位置编码也是在变化的，那”I eat apple”和”I eat apple.”的位置编码就不同了（后者多了一个’.’，线性插值间隔就不一样了），而实际上他们的语意都是相同的，我们也不希望这两个句子在位置编码上不同。</p>
<p>那么有没有办法能让每个位置的值是个固定的值，且满足前面提到的要求呢？</p>
<h5 id="二进制位置编码（Binary-Position-Encoding）"><a href="#二进制位置编码（Binary-Position-Encoding）" class="headerlink" title="二进制位置编码（Binary Position Encoding）"></a>二进制位置编码（Binary Position Encoding）</h5><p>在查询的过程中，往往Q不是1维的向量，而是有t行d列的向量嵌入(embeddings)。比如身高查询的时候假设我们以0.1为粒度，范围定在[0,250]，那么就能拆分出25011个点，每个查询值可以是一个one-hot向量。举个例子，查询172.5，对应向量1725索引处是1，其他位置是0的一个one hot向量。然后one hot空间太过稀疏，再用PCA或者Autoencoder等方法将大小为25011的空间压缩到一个自己需要的维度。</p>
<p>这里，假设我们查询压缩到两个维度，那么对于4条查询，可以仿照二进制编码，把每个位置都进行编码：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Q</th>
<th style="text-align:center">Emb1</th>
<th style="text-align:center">Emb2</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Q1</td>
<td style="text-align:center">0</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">Q2</td>
<td style="text-align:center">0</td>
<td style="text-align:center">1</td>
</tr>
<tr>
<td style="text-align:center">Q3</td>
<td style="text-align:center">1</td>
<td style="text-align:center">0</td>
</tr>
<tr>
<td style="text-align:center">Q4</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
</tr>
</tbody>
</table>
</div>
<p>要压缩到的空间（其实就是embedding空间）的密度可以自己更改，这里使用的是长度为2，理论上最长可以到25011，那就可以编码最长$2^{25011}$个位置，这是妥妥的够用的，每个位置填什么也就解决了。</p>
<p>虽然这样编码能给每个位置分配一个固定的值，但是他也带来了新的问题：</p>
<ol>
<li>这样编码过于离散，只是能再高维四面体上取顶点0或1（这个是我从二维和三维图像中总结出来的，高维我也想象不出来）</li>
<li>对同一行而言，列与列之间太像了，容易遇上非常多个连续的0或1，导致神经网络很难找到这个空间的正交基（基向量之间相关性太强）<br>那下面我们将一步一步解决这两个问题。</li>
</ol>
<h5 id="正余弦位置编码"><a href="#正余弦位置编码" class="headerlink" title="正余弦位置编码"></a>正余弦位置编码</h5><p>可以使用正弦函数来为每个位置的编码，从而利用[-1,1]这个实数域上的每一个点：</p>
<script type="math/tex; mode=display">
 PE_{t,d} = sin(\frac{t}{2^{d-1}})</script><p>$PE_{t,d}$表示第t行第d列的位置编码，对应第$Q_t$和$Emb_d$。这样做可以解决第一个问题。此外，不同列之间的波长不同，波长不同正弦函数对t的敏感程度不同，也能帮助找到正交基。</p>
<p>到这里还没有结束，仔细观察这个函数，不难看出第i列的波长$\lambda=2^d\pi$。如果Embedding只是有两个维度，那两个维度对位置的编码是一个首尾相接的线，形状如下图。<br><img src="/images/2024-10-25-QKV奇思小记/image-2.png" alt="alt text"><br>拓展到三维也是一样的封闭的环，这样的现象是因为正弦函数是周期性变化的。那就意味着，当Q的长度逐渐变大，就容易出现其中两行查询所对应的位置编码相同的情况，这显然不是我们希望看到的。</p>
<h5 id="正余弦位置编码改进"><a href="#正余弦位置编码改进" class="headerlink" title="正余弦位置编码改进"></a>正余弦位置编码改进</h5><p>那也就顺理成章的能够想到，既然是因为周期变化导致的，那我让它<strong>周期足够大</strong>不就行了。这个思路是对的，核心就是拉大波长来减少周期性的问题，可以通过下面两个方式：</p>
<ol>
<li>让embedding的空间足够大，这样的话最大的d对应的波长$2^d\pi$就足够大，比如要应付最大长度1000的Q，那只需要$d \gt log(\frac{1000}{\pi}) \approx 5.763$就行了。</li>
<li>同样是拉大波长，我们使用$10000^d\pi$作为周长，这么长的波长，就不容易遇到两行完全相同的情况了</li>
</ol>
<p>两个方案实际上都是可行的，对于前者，按照通义千问的embedding维度长度3584，如果使用前者，理论上它能够保证填$2^{3584}$数量级的query，不过模型什么时候能学到这一信息（收敛）也是有不同的，但是我不太懂这个方面的内容就不胡诌了。后者的话更甚之，在前者的基础上，同时把波长的底数从2变成10000，能够进一步改善周期性的问题（其实前者本身应该也是够用）</p>
<p>而这一编码方式，也是Attention is All You Need这篇文章所使用的，他只简短的用一小段提了一下，他使用的编码方式如下：</p>
<script type="math/tex; mode=display">
PE_{t,d} = 
 \begin{cases}
 sin(\frac{t}{10000^{\frac{2i}{d}}}), \quad d=2i\\
 cos(\frac{t}{10000^{\frac{2i}{d}}}), \quad d=2i+1
 \end{cases}</script><p>可以注意到，于我提到的编码方式不同的是，文章中还将相邻的两行偏移了$\frac{1}{4}$个波长（sin变cos）。这样做首先保证了两行（两个query）之间在值上的相邻，因为是同一个波函数的不同位置，且波长很长，两点相对接近。此外，它也让列于列之间有了显著的不同(对于相邻列sin和cos的相关性非常低，而每两列是同一个波长，i变化的时候波长也变了同时位移也变化了)，降低了列之间的相关性。</p>
<p>而列于列之间的相关性低，能够让神经网络更好的分解这一空间并找到对应数量的相对正交的基向量，从而理解这一个位置编码空间，进而理解Q不同行之间的位置关系。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文主要从一次偶然的交流出发，探究了如何从一张KV表中查Q对应的值。在此基础上，推广了Q在多值情况下的做法，以及讨论了Q不同行之间的位置关系。然后探讨了如何“表示”位置，以及为什么Transformer的位置是用这么一个奇怪的公式表示的。</p>
<script type="math/tex; mode=display">
PE_{t,d} = 
 \begin{cases}
 sin(\frac{t}{10000^{\frac{2i}{d}}}), \quad d=2i\\
 cos(\frac{t}{10000^{\frac{2i}{d}}}), \quad d=2i+1
 \end{cases}</script>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">chuanmx</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">chuanmx</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  













<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'forest',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>

